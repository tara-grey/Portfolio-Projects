{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee8de1d5-d2ea-4eb5-b86a-ef5876c6b08c",
   "metadata": {},
   "source": [
    "By: Emilie Baek (mei558), Makenzie Kalebick (poq312), Riya Patel (lkq587), Tara Grey (aao402)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f44960e-3fae-41f3-90ae-f8ab5e27cca3",
   "metadata": {},
   "source": [
    "# Agreement Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4cb063-37b9-4aaa-95a6-dda67a676705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "technology1 = []\n",
    "transport1 = []\n",
    "cybersecurity1 = []\n",
    "governance1 = []\n",
    "products1 = []\n",
    "infrastructure1 = []\n",
    "utilities1 = []\n",
    "connectivity1 = []\n",
    "\n",
    "with open('predictors_annotation.csv','r',encoding='utf-8') as iFile:\n",
    "    csv_reader = csv.reader(iFile)\n",
    "    next(csv_reader)\n",
    "    \n",
    "    rows = list(csv_reader) \n",
    "    \n",
    "    technology1 = [row[2] for row in rows]\n",
    "    transport1 = [row[3] for row in rows]\n",
    "    cybersecurity1 = [row[4] for row in rows]\n",
    "    governance1 = [row[5] for row in rows]\n",
    "    products1 = [row[6] for row in rows]\n",
    "    infrastructure1 = [row[7] for row in rows]\n",
    "    utilities1 = [row[8] for row in rows]\n",
    "    connectivity1 = [row[8] for row in rows]\n",
    "    \n",
    "#print(technology)\n",
    "#print(transport)\n",
    "#print(cybersecurity)\n",
    "#print(governance)\n",
    "#print(products)\n",
    "#print(infrastructure)\n",
    "#print(utilities1)\n",
    "#print(connectivity1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56739788-12a6-4f8f-9a61-33052471fb2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "technology2 = []\n",
    "transport2 = []\n",
    "cybersecurity2 = []\n",
    "governance2 = []\n",
    "products2 = []\n",
    "infrastructure2 = []\n",
    "utilities2 = []\n",
    "connectivity2 = []\n",
    "\n",
    "with open('thrashers_annotation.csv', 'r', encoding='utf-8') as iFile:\n",
    "    csv_reader = csv.reader(iFile)\n",
    "    next(csv_reader)\n",
    "    \n",
    "    rows = list(csv_reader)\n",
    "    \n",
    "    technology2 = [row[2] for row in rows]\n",
    "    transport2 = [row[3] for row in rows]\n",
    "    cybersecurity2 = [row[4] for row in rows]\n",
    "    governance2 = [row[5] for row in rows]\n",
    "    products2 = [row[6] for row in rows]\n",
    "    infrastructure2 = [row[7] for row in rows]\n",
    "    utilities2 = [row[8] for row in rows]\n",
    "    connectivity2 = [row[9] for row in rows]\n",
    "    \n",
    "#print(technology2)\n",
    "#print(transport2)\n",
    "#print(cybersecurity2)\n",
    "#print(governance2)\n",
    "#print(products2)\n",
    "#print(infrastructure2)\n",
    "#print(utilities2)\n",
    "#print(connectivity2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d78a64-4905-447f-aa45-9bee41390931",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score for Technology: 0.20\n",
      "Kappa Score for Transport: 0.17\n",
      "Kappa Score for Cybersecurity: 0.06\n",
      "Kappa Score for Governance: 0.29\n",
      "Kappa Score for Products: 0.11\n",
      "Kappa Score for Infrastructure: 0.12\n",
      "Kappa Score for Utilities: 0.34\n",
      "Kappa Score for Connectivity: -0.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "print(f\"Kappa Score for Technology: {cohen_kappa_score(technology1, technology2):.2f}\")\n",
    "print(f\"Kappa Score for Transport: {cohen_kappa_score(transport1, transport2):.2f}\")\n",
    "print(f\"Kappa Score for Cybersecurity: {cohen_kappa_score(cybersecurity1, cybersecurity2):.2f}\")\n",
    "print(f\"Kappa Score for Governance: {cohen_kappa_score(governance1, governance2):.2f}\")\n",
    "print(f\"Kappa Score for Products: {cohen_kappa_score(products1, products2):.2f}\")\n",
    "print(f\"Kappa Score for Infrastructure: {cohen_kappa_score(infrastructure1, infrastructure2):.2f}\")\n",
    "print(f\"Kappa Score for Utilities: {cohen_kappa_score(utilities1, utilities2):.2f}\")\n",
    "print(f\"Kappa Score for Connectivity: {cohen_kappa_score(connectivity1, connectivity2):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e89be7-f827-494f-a5bd-af4d6e16ce09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a54de6b1-957a-4fe6-a23d-12bf212414ae",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99e483ec-d584-4d29-8c98-974933459e0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Importing Functions ###\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b293b36-47cb-4287-af3c-c6bf573b9e75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "### Initializing X_text and Y and Appending to Data ###\n",
    "\n",
    "X_text = []\n",
    "y = []\n",
    "\n",
    "with open('annotations.csv', encoding='utf-8') as myFile:\n",
    "    iCSV = csv.reader(myFile)\n",
    "    header = next(iCSV)\n",
    "    \n",
    "    for row in iCSV:\n",
    "        if len(row) >= 2:\n",
    "            X_text.append(row[1])\n",
    "            y.append([int(row[2]), int(row[3]), int(row[4]), int(row[5])])\n",
    "\n",
    "print(len(y), len(X_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886a55c4-17ec-45d6-8f31-65cdc078bcfa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model 1: No Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba9aee97-59de-440b-86a6-ccd34106c5f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Target Variable 1:\n",
      "Accuracy: 0.6340\n",
      "Precision: 0.5327\n",
      "Recall: 0.3000\n",
      "F1 Score Micro: 0.3838\n",
      "F1 Score Macro: 0.5618\n",
      "\n",
      "Metrics for Target Variable 2:\n",
      "Accuracy: 0.9420\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score Micro: 0.0000\n",
      "F1 Score Macro: 0.4851\n",
      "\n",
      "Metrics for Target Variable 3:\n",
      "Accuracy: 0.9400\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score Micro: 0.0000\n",
      "F1 Score Macro: 0.4845\n",
      "\n",
      "Metrics for Target Variable 4:\n",
      "Accuracy: 0.9780\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score Micro: 0.0000\n",
      "F1 Score Macro: 0.4944\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(X_text)\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "preds = []\n",
    "count = 0\n",
    "\n",
    "# Training and predicting for each target variable separately\n",
    "for i in range(4):\n",
    "    # Training the model\n",
    "    classifier = LinearSVC()\n",
    "    classifier.fit(X_train, [row[i] for row in y_train])\n",
    "    \n",
    "    # Predicting on the test set\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluation Metrics\n",
    "    accuracy = accuracy_score([row[i] for row in y_test], y_pred)\n",
    "    precision = precision_score([row[i] for row in y_test], y_pred)\n",
    "    recall = recall_score([row[i] for row in y_test], y_pred)\n",
    "    f1_micro = f1_score([row[i] for row in y_test], y_pred)\n",
    "    f1_macro = f1_score([row[i] for row in y_test], y_pred, average='macro') \n",
    "    \n",
    "    y_pred = classifier.predict(X)\n",
    "\n",
    "    \n",
    "    p = 0\n",
    "    while p < len(y_pred):\n",
    "        if count == 0:\n",
    "            preds += [[]]\n",
    "        p += 1\n",
    "    p = 0\n",
    "    while p < len(y_pred):\n",
    "        preds[p] += [y_pred[p]]\n",
    "        \n",
    "        p += 1\n",
    "    count += 1\n",
    "        \n",
    "\n",
    "    print(f\"\\nMetrics for Target Variable {i + 1}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score Micro: {f1_micro:.4f}\")\n",
    "    print(f\"F1 Score Macro: {f1_macro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca621824-8544-4b74-8ee0-5173ce0bbadb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model 2: No Features, Balanced Class Weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d452a2d-001d-48b1-84f2-ef9a87e5af09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Target Variable 1:\n",
      "Accuracy: 0.6350\n",
      "Precision: 0.5517\n",
      "Recall: 0.4051\n",
      "F1 Score Micro: 0.4672\n",
      "F1 Score Macro: 0.5948\n",
      "\n",
      "Metrics for Target Variable 2:\n",
      "Accuracy: 0.9450\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score Micro: 0.0000\n",
      "F1 Score Macro: 0.4859\n",
      "\n",
      "Metrics for Target Variable 3:\n",
      "Accuracy: 0.9400\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score Micro: 0.0000\n",
      "F1 Score Macro: 0.4845\n",
      "\n",
      "Metrics for Target Variable 4:\n",
      "Accuracy: 0.9800\n",
      "Precision: 1.0000\n",
      "Recall: 0.2000\n",
      "F1 Score Micro: 0.3333\n",
      "F1 Score Macro: 0.6616\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(X_text)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "preds = []\n",
    "count = 0\n",
    "\n",
    "for i in range(4):\n",
    "    # Modifying class weights \n",
    "    class_weights = \"balanced\"\n",
    "    \n",
    "    # Training the model with class weights\n",
    "    classifier = LinearSVC(class_weight=class_weights)\n",
    "    classifier.fit(X_train, [row[i] for row in y_train])\n",
    "    \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score([row[i] for row in y_test], y_pred)\n",
    "    precision = precision_score([row[i] for row in y_test], y_pred)\n",
    "    recall = recall_score([row[i] for row in y_test], y_pred)\n",
    "    f1_micro = f1_score([row[i] for row in y_test], y_pred)\n",
    "    f1_macro = f1_score([row[i] for row in y_test], y_pred, average='macro') \n",
    "    \n",
    "    y_pred = classifier.predict(X)\n",
    "\n",
    "    \n",
    "    p = 0\n",
    "    while p < len(y_pred):\n",
    "        if count == 0:\n",
    "            preds += [[]]\n",
    "        p += 1\n",
    "    p = 0\n",
    "    while p < len(y_pred):\n",
    "        preds[p] += [y_pred[p]]\n",
    "        \n",
    "        p += 1\n",
    "    count += 1\n",
    "\n",
    "    print(f\"\\nMetrics for Target Variable {i + 1}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score Micro: {f1_micro:.4f}\")\n",
    "    print(f\"F1 Score Macro: {f1_macro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd651a2-a1dd-4b8e-acce-89988e511a51",
   "metadata": {},
   "source": [
    "## Model 3: Technology Keywords Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c369bdc5-1d5a-4c60-a061-9862450e8846",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Target Variable 1:\n",
      "Accuracy: 0.6300\n",
      "Precision: 0.5439\n",
      "Recall: 0.3924\n",
      "F1 Score Micro: 0.4559\n",
      "F1 Score Macro: 0.5878\n",
      "\n",
      "Metrics for Target Variable 2:\n",
      "Accuracy: 0.9450\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score Micro: 0.0000\n",
      "F1 Score Macro: 0.4859\n",
      "\n",
      "Metrics for Target Variable 3:\n",
      "Accuracy: 0.9500\n",
      "Precision: 0.5000\n",
      "Recall: 0.1000\n",
      "F1 Score Micro: 0.1667\n",
      "F1 Score Macro: 0.5704\n",
      "\n",
      "Metrics for Target Variable 4:\n",
      "Accuracy: 0.9750\n",
      "Precision: 0.5000\n",
      "Recall: 0.2000\n",
      "F1 Score Micro: 0.2857\n",
      "F1 Score Macro: 0.6365\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Defining Technology-Related Keywords\n",
    "technology_keywords = ['technology', 'innovation', 'ai', 'machine learning', 'bitcoin', 'Internet',\n",
    "                       'Programming', 'Data', 'Network', 'Robotics', 'smartphone', 'Digital', 'Apple',\n",
    "                       'Youtube', 'Netflix', 'Hulu', 'Amazon', 'Online', 'Disney', 'Facebook', 'instagram',\n",
    "                       'Zoom', 'twitter','saw', 'electrocuted', 'crypto','vacuum', 'Artificial Intelligence',\n",
    "                       'ML', 'Python', 'R', 'SAS']\n",
    "\n",
    "# Creating Feature: Count of Technology Keywords\n",
    "X_technology_keywords_count = np.array([[text.lower().count(keyword) for keyword in technology_keywords] for text in X_text])\n",
    "\n",
    "\n",
    "X_w_count= vec.fit_transform(X_text)\n",
    "\n",
    "X_combined = sp.hstack([X_w_count, sp.csr_matrix(X_technology_keywords_count)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "count = 0\n",
    "preds = []\n",
    "\n",
    "for i in range(4):\n",
    "    class_weights = \"balanced\"\n",
    "    \n",
    "    classifier = LinearSVC(class_weight=class_weights)\n",
    "    classifier.fit(X_train, [row[i] for row in y_train])\n",
    "    \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score([row[i] for row in y_test], y_pred)\n",
    "    precision = precision_score([row[i] for row in y_test], y_pred)\n",
    "    recall = recall_score([row[i] for row in y_test], y_pred)\n",
    "    f1_micro = f1_score([row[i] for row in y_test], y_pred)\n",
    "    f1_macro = f1_score([row[i] for row in y_test], y_pred, average='macro')\n",
    "\n",
    "    print(f\"\\nMetrics for Target Variable {i + 1}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score Micro: {f1_micro:.4f}\")\n",
    "    print(f\"F1 Score Macro: {f1_macro:.4f}\")\n",
    "    \n",
    "    y_pred = classifier.predict(X_combined)\n",
    "\n",
    "    p = 0\n",
    "    while p < len(y_pred):\n",
    "        if count == 0:\n",
    "            preds += [[]]\n",
    "        p += 1\n",
    "    p = 0\n",
    "    while p < len(y_pred):\n",
    "        preds[p] += [y_pred[p]]\n",
    "        \n",
    "        p += 1\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bd8c0c-e320-4747-a6de-ed2007f0b421",
   "metadata": {},
   "source": [
    "## Model 4: Uppercase Word Count Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05ddefcb-fb3e-40df-94b2-76068bbfbf04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Target Variable 1:\n",
      "Accuracy: 0.6350\n",
      "Precision: 0.5517\n",
      "Recall: 0.4051\n",
      "F1 Score Micro: 0.4672\n",
      "F1 Score Macro: 0.5948\n",
      "\n",
      "Metrics for Target Variable 2:\n",
      "Accuracy: 0.9450\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score Micro: 0.0000\n",
      "F1 Score Macro: 0.4859\n",
      "\n",
      "Metrics for Target Variable 3:\n",
      "Accuracy: 0.9400\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score Micro: 0.0000\n",
      "F1 Score Macro: 0.4845\n",
      "\n",
      "Metrics for Target Variable 4:\n",
      "Accuracy: 0.9800\n",
      "Precision: 1.0000\n",
      "Recall: 0.2000\n",
      "F1 Score Micro: 0.3333\n",
      "F1 Score Macro: 0.6616\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "\n",
    "# Feature: Count of uppercase words\n",
    "X_uppercase_count = np.array([[sum(1 for word in text.split() if word.isupper())] for text in X_text])\n",
    "\n",
    "X_w_count= vec.fit_transform(X_text)\n",
    "\n",
    "X_combined = sp.hstack([X_w_count, sp.csr_matrix(X_uppercase_count)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "count = 0\n",
    "preds = []\n",
    "\n",
    "for i in range(4):\n",
    "    class_weights = \"balanced\"  \n",
    "    \n",
    "    classifier = LinearSVC(class_weight=class_weights)\n",
    "    classifier.fit(X_train, [row[i] for row in y_train])\n",
    "\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score([row[i] for row in y_test], y_pred)\n",
    "    precision = precision_score([row[i] for row in y_test], y_pred)\n",
    "    recall = recall_score([row[i] for row in y_test], y_pred)\n",
    "    f1_micro = f1_score([row[i] for row in y_test], y_pred)\n",
    "    f1_macro = f1_score([row[i] for row in y_test], y_pred, average='macro') \n",
    "\n",
    "    print(f\"\\nMetrics for Target Variable {i + 1}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score Micro: {f1_micro:.4f}\")\n",
    "    print(f\"F1 Score Macro: {f1_macro:.4f}\")\n",
    "    \n",
    "    y_pred = classifier.predict(X_combined)\n",
    "\n",
    "    p = 0\n",
    "    while p < len(y_pred):\n",
    "        if count == 0:\n",
    "            preds += [[]]\n",
    "        p += 1\n",
    "    p = 0\n",
    "    while p < len(y_pred):\n",
    "        preds[p] += [y_pred[p]]\n",
    "        \n",
    "        p += 1\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa5d250-706a-4c3b-a873-43a3a04ed248",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model 5: Punctuation Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "551b2f0e-a7ea-4af8-a544-5c8509568c12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Target Variable 1:\n",
      "Accuracy: 0.6550\n",
      "Precision: 0.5962\n",
      "Recall: 0.3924\n",
      "F1 Score Micro: 0.4733\n",
      "F1 Score Macro: 0.6084\n",
      "\n",
      "Metrics for Target Variable 2:\n",
      "Accuracy: 0.9450\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score Micro: 0.0000\n",
      "F1 Score Macro: 0.4859\n",
      "\n",
      "Metrics for Target Variable 3:\n",
      "Accuracy: 0.9300\n",
      "Precision: 0.1667\n",
      "Recall: 0.1000\n",
      "F1 Score Micro: 0.1250\n",
      "F1 Score Macro: 0.5443\n",
      "\n",
      "Metrics for Target Variable 4:\n",
      "Accuracy: 0.9750\n",
      "Precision: 0.5000\n",
      "Recall: 0.2000\n",
      "F1 Score Micro: 0.2857\n",
      "F1 Score Macro: 0.6365\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Feature: Count of Punctuation Marks\n",
    "punctuation_marks = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "\n",
    "X_punctuation_counts = np.array([[text.count(punctuation) for punctuation in punctuation_marks] for text in X_text])\n",
    "\n",
    "X_w_count= vec.fit_transform(X_text)\n",
    "\n",
    "X_combined = sp.hstack([X_w_count, sp.csr_matrix(X_punctuation_counts)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "count = 0\n",
    "final_preds = []\n",
    "\n",
    "for i in range(4):\n",
    "    class_weights = \"balanced\"\n",
    "    \n",
    "    classifier = LinearSVC(class_weight=class_weights)\n",
    "    classifier.fit(X_train, [row[i] for row in y_train])\n",
    "    \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score([row[i] for row in y_test], y_pred)\n",
    "    precision = precision_score([row[i] for row in y_test], y_pred)\n",
    "    recall = recall_score([row[i] for row in y_test], y_pred)\n",
    "    f1_micro = f1_score([row[i] for row in y_test], y_pred)\n",
    "    f1_macro = f1_score([row[i] for row in y_test], y_pred, average='macro') \n",
    "\n",
    "    print(f\"\\nMetrics for Target Variable {i + 1}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score Micro: {f1_micro:.4f}\")\n",
    "    print(f\"F1 Score Macro: {f1_macro:.4f}\")\n",
    "    \n",
    "    y_pred = classifier.predict(X_combined)\n",
    "\n",
    "    p = 0\n",
    "    while p < len(y_pred):\n",
    "        if count == 0:\n",
    "            final_preds += [[]]\n",
    "        p += 1\n",
    "    p = 0\n",
    "    while p < len(y_pred):\n",
    "        final_preds[p] += [y_pred[p]]\n",
    "        \n",
    "        p += 1\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24618bd1-4737-4ada-855c-ebcfe8e213e8",
   "metadata": {},
   "source": [
    "## Model 6: All Three Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea972de8-6664-462c-af58-ce2f5adc7a40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Target Variable 1:\n",
      "Accuracy: 0.6300\n",
      "Precision: 0.5439\n",
      "Recall: 0.3924\n",
      "F1 Score Micro: 0.4559\n",
      "F1 Score Macro: 0.5878\n",
      "\n",
      "Metrics for Target Variable 2:\n",
      "Accuracy: 0.9400\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score Micro: 0.0000\n",
      "F1 Score Macro: 0.4845\n",
      "\n",
      "Metrics for Target Variable 3:\n",
      "Accuracy: 0.9300\n",
      "Precision: 0.2500\n",
      "Recall: 0.2000\n",
      "F1 Score Micro: 0.2222\n",
      "F1 Score Macro: 0.5928\n",
      "\n",
      "Metrics for Target Variable 4:\n",
      "Accuracy: 0.9650\n",
      "Precision: 0.2500\n",
      "Recall: 0.2000\n",
      "F1 Score Micro: 0.2222\n",
      "F1 Score Macro: 0.6022\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Defining Technology-Related Keywords\n",
    "technology_keywords = ['technology', 'innovation', 'ai', 'machine learning', 'bitcoin', 'Internet',\n",
    "                   \t'Programming', 'Data', 'Network', 'Robotics', 'smartphone', 'Digital', 'Apple',\n",
    "                   \t'Youtube', 'Netflix', 'Hulu', 'Amazon', 'Online', 'Disney', 'Facebook', 'instagram',\n",
    "                   \t'Zoom', 'twitter', 'saw', 'electrocuted', 'crypto', 'vacuum', 'Artificial Intelligence',\n",
    "                   \t'ML', 'Python', 'R', 'SAS']\n",
    "\n",
    "# Feature: Count of Technology Keywords\n",
    "X_technology_keywords_count = np.array([[text.lower().count(keyword) for keyword in technology_keywords] for text in X_text])\n",
    "\n",
    "# Feature: Count of Uppercase words\n",
    "X_uppercase_count = np.array([[sum(1 for word in text.split() if word.isupper())] for text in X_text])\n",
    "\n",
    "# Feature: Count of Punctuation Marks\n",
    "punctuation_marks = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';',\n",
    "                     '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "\n",
    "X_punctuation_counts = np.array([[text.count(punctuation) for punctuation in punctuation_marks] for text in X_text])\n",
    "\n",
    "\n",
    "\n",
    "X_w_count= vec.fit_transform(X_text)\n",
    "X_combined = sp.hstack([X_w_count, sp.csr_matrix(X_technology_keywords_count), sp.csr_matrix(X_uppercase_count), sp.csr_matrix(X_punctuation_counts)])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "count = 0\n",
    "preds = []\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "for i in range(4):\n",
    "    class_weights = \"balanced\"  \n",
    "    \n",
    "    classifier = LinearSVC(class_weight=class_weights)\n",
    "    classifier.fit(X_train, [row[i] for row in y_train])\n",
    "    \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score([row[i] for row in y_test], y_pred)\n",
    "    precision = precision_score([row[i] for row in y_test], y_pred)\n",
    "    recall = recall_score([row[i] for row in y_test], y_pred)\n",
    "    f1_micro = f1_score([row[i] for row in y_test], y_pred)\n",
    "    f1_macro = f1_score([row[i] for row in y_test], y_pred, average='macro')\n",
    "\n",
    "    print(f\"\\nMetrics for Target Variable {i + 1}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score Micro: {f1_micro:.4f}\")\n",
    "    print(f\"F1 Score Macro: {f1_macro:.4f}\")\n",
    "\n",
    "    y_pred = classifier.predict(X_combined)\n",
    "\n",
    "    p = 0\n",
    "    while p < len(y_pred):\n",
    "        if count == 0:\n",
    "            preds += [[]]\n",
    "        p += 1\n",
    "    p = 0\n",
    "    while p < len(y_pred):\n",
    "        preds[p] += [y_pred[p]]\n",
    "        \n",
    "        p += 1\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0187d0-f326-4063-b515-a481d07ce695",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1f680c4-f0b0-4a2b-81b5-109d2be00f88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Target Variable 1 with feature selection using SelectKBest:\n",
      "Accuracy: 0.4650\n",
      "Precision: 0.4000\n",
      "Recall: 0.7089\n",
      "F1 Score: 0.5114\n",
      "F1 Score Macro: 0.4601\n",
      "Selected Features: [8777 8787 8789 8790 8791 8792 8793 8796 8800 8802]\n",
      "\n",
      "Metrics for Target Variable 2 with feature selection using SelectKBest:\n",
      "Accuracy: 0.9600\n",
      "Precision: 1.0000\n",
      "Recall: 0.2727\n",
      "F1 Score: 0.4286\n",
      "F1 Score Macro: 0.7039\n",
      "Selected Features: [ 536 2606 2732 4783 4873 5704 6615 7057 8266 8666]\n",
      "\n",
      "Metrics for Target Variable 3 with feature selection using SelectKBest:\n",
      "Accuracy: 0.7650\n",
      "Precision: 0.1224\n",
      "Recall: 0.6000\n",
      "F1 Score: 0.2034\n",
      "F1 Score Macro: 0.5328\n",
      "Selected Features: [5478 7753 8745 8778 8779 8787 8789 8790 8791 8804]\n",
      "\n",
      "Metrics for Target Variable 4 with feature selection using SelectKBest:\n",
      "Accuracy: 0.9650\n",
      "Precision: 0.2500\n",
      "Recall: 0.2000\n",
      "F1 Score: 0.2222\n",
      "F1 Score Macro: 0.6022\n",
      "Selected Features: [3736 5076 8407 8747 8768 8777 8789 8790 8791 8792]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "k_best = SelectKBest(chi2, k=10)\n",
    "for i in range(4):\n",
    "    class_for_weights = \"balanced\"\n",
    "    classifier = LinearSVC(class_weight=class_for_weights)\n",
    "    classifier.fit(X_train, [row[i] for row in y_train])\n",
    "    y_train_current = [row[i] for row in y_train]\n",
    "\n",
    "    X_train_selected = k_best.fit_transform(X_train, y_train_current)\n",
    "    selected_indices = k_best.get_support(indices=True)\n",
    "\n",
    "    X_train_selected_original = X_train[:, selected_indices]\n",
    "    X_test_selected_original = X_test[:, selected_indices]\n",
    "    classifier.fit(X_train_selected_original, y_train_current)\n",
    "    y_pred = classifier.predict(X_test_selected_original)\n",
    "    accuracy = accuracy_score([row[i] for row in y_test], y_pred)\n",
    "    precision = precision_score([row[i] for row in y_test], y_pred)\n",
    "    recall = recall_score([row[i] for row in y_test], y_pred)\n",
    "    f1 = f1_score([row[i] for row in y_test], y_pred)\n",
    "    f1_macro = f1_score([row[i] for row in y_test], y_pred, average='macro')\n",
    "  \n",
    "    print(f\"\\nMetrics for Target Variable {i + 1} with feature selection using SelectKBest:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"F1 Score Macro: {f1_macro:.4f}\")\n",
    "    \n",
    "    print(\"Selected Features:\", selected_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed465211-e1a7-42f8-aeb3-351650319843",
   "metadata": {},
   "source": [
    "## Creating CSV: Text, Ground Truth, and Best Model (5) Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8971479e-6ed0-48b8-8272-f3d4bb5f80e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "with open('ML_Model_Predictions.csv', 'w', newline='', encoding='utf-8') as oFile:\n",
    "    oCSV = csv.writer(oFile, delimiter=',')\n",
    "    header = ['Text','Tech','Tech Prediction', 'Transport', 'Transport Prediction','Governance',' Governance Prediction', 'Public Utilities', 'Utilities Prediction' ]\n",
    "    oCSV.writerow(header)    \n",
    "    for txt, t_row, p_row in zip(X_text, y, final_preds):\n",
    "        new_t = [str(x) for x in t_row]\n",
    "        new_p = [str(int(x)) for x in p_row]\n",
    "        output = [txt] + new_t + new_p\n",
    "        oCSV.writerow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c984e-c666-49a5-9390-ee50d60419d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
